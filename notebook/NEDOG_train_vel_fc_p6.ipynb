{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qcGZJeA0AhR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYuFBXQF0Lta"
      },
      "outputs": [],
      "source": [
        "# HOMEディレクトリ設定(環境に合わせて変更してください)\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Google Colaboratoryの場合\n",
        "    HOME = Path(\"/content/drive/MyDrive/signate/NEDOG\")\n",
        "\n",
        "    # Google Driveをマウント\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    # それ以外\n",
        "    HOME = Path(\"..\")\n",
        "\n",
        "# INPUT/WORKINGディレクトリ設定\n",
        "INPUT = HOME / \"input\"\n",
        "WORKING = HOME / \"working\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09Tz3wQ-xzR2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore', FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2g8oh9R0SKm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import scipy.signal\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRwB5yC10SwF"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_a_G4zC-YNt"
      },
      "source": [
        "# データ読み取り"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DuYGk2m-hoH"
      },
      "outputs": [],
      "source": [
        "# スムージング関数\n",
        "def smoothing(df, col):\n",
        "    icol = df.columns.get_loc(col)\n",
        "    for i in range(len(df)//30):\n",
        "        df.iloc[i*30:(i+1)*30,icol] = scipy.signal.savgol_filter(\n",
        "            df.iloc[i*30:(i+1)*30,icol].values,9,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfrHz_DUO4fu"
      },
      "outputs": [],
      "source": [
        "tr_emg_df = pd.read_pickle(WORKING / 'prep2_tr_emg.pickle')\n",
        "tr_vel_df = pd.read_pickle(WORKING / 'prep2_tr_vel.pickle')\n",
        "ts_emg_df = pd.read_pickle(WORKING / 'prep2_ts_emg.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# スロープ上り下り区別用フラグ(均等にするため閾値は-10とする)\n",
        "tr_dir_df = tr_vel_df.groupby(['subject','trial'])['vel_x'].apply(\n",
        "    lambda x: x.sum() > -10\n",
        ").rename('dir').reset_index()\n",
        "tr_vel_df = tr_vel_df.merge(tr_dir_df, on=['subject','trial'])"
      ],
      "metadata": {
        "id": "2UJjVG-i88-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2s3sSSms834"
      },
      "outputs": [],
      "source": [
        "# 加速度推論結果\n",
        "tr_pred_acc_df = pd.read_pickle(WORKING / 'tr_pred_acc.pickle')\n",
        "for col in tr_pred_acc_df.columns:\n",
        "    smoothing(tr_pred_acc_df, col)\n",
        "ts_pred_acc_df = pd.read_pickle(WORKING / 'ts_pred_acc.pickle')\n",
        "for col in ts_pred_acc_df.columns:\n",
        "    smoothing(ts_pred_acc_df, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTy0_TfntZKj"
      },
      "outputs": [],
      "source": [
        "# 鏡像データ判定\n",
        "tr_pred_df = pd.concat([tr_emg_df[['subject','trial']], tr_pred_acc_df], axis=1)\n",
        "ts_pred_df = pd.concat([ts_emg_df[['subject','trial']], ts_pred_acc_df], axis=1)\n",
        "tr_pred_df['mirror'] = tr_pred_df.index >= len(tr_pred_acc_df)//2\n",
        "ts_pred_df['mirror'] = ts_pred_df.index >= len(ts_pred_acc_df)//2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCmDH2c_Os2y"
      },
      "outputs": [],
      "source": [
        "# ラグ特徴量取得関数\n",
        "def add_lag_feature(df, lags):\n",
        "    dfs = [df]\n",
        "    for lag in lags:\n",
        "        lag_df = df.groupby(['subject','trial','mirror']).shift(lag).rename(\n",
        "            columns=lambda x: f'lag{lag} '+x\n",
        "        )\n",
        "        dfs.append(lag_df)\n",
        "    return pd.concat(dfs, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oj_XPwROssm"
      },
      "outputs": [],
      "source": [
        "# ラグ特徴量\n",
        "tr_pred_df = add_lag_feature(\n",
        "    tr_pred_df, list(range(-1,-30,-4))+list(range(1,30,4))\n",
        ").drop(['subject','trial','mirror'], axis=1)\n",
        "ts_pred_df = add_lag_feature(\n",
        "    ts_pred_df, list(range(-1,-30,-4))+list(range(1,30,4))\n",
        ").drop(['subject','trial','mirror'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7OV1ygCkWd0"
      },
      "outputs": [],
      "source": [
        "tr_emg_df = pd.concat([tr_emg_df,tr_pred_df], axis=1)\n",
        "ts_emg_df = pd.concat([ts_emg_df,ts_pred_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3K1zUd0Pm7A"
      },
      "source": [
        "# 学習/推論用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOesA7chGyXJ"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'verbosity': -1,\n",
        "    'random_state': 41,\n",
        "    'learning_rate': 0.05,\n",
        "    'boosting_type': 'gbdt',\n",
        "\n",
        "    #'lambda_l1': 0.5103247316877605, 'lambda_l2': 0.015010593003038865,\n",
        "    #'num_leaves': 58, 'feature_fraction': 0.474875745099162,\n",
        "    #'bagging_fraction': 0.8775197693430815, 'bagging_freq': 9, 'min_child_samples': 22\n",
        "    'lambda_l1': 1.832631928461453, 'lambda_l2': 0.41989974140767805,\n",
        "    'num_leaves': 162, 'feature_fraction': 0.3696427245878058,\n",
        "    'bagging_fraction': 0.957520455198611, 'bagging_freq': 5, 'min_child_samples': 77\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFSwzhD3Pmb0"
      },
      "outputs": [],
      "source": [
        "# holdoutで学習/推論\n",
        "def train_holdout(x_train, y_train, x_valid, y_valid, params):\n",
        "    data_train = lgb.Dataset(data=x_train, label=y_train)\n",
        "    data_valid = lgb.Dataset(data=x_valid, label=y_valid)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params, data_train, valid_sets=[data_valid],\n",
        "        num_boost_round=10000,\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=100, verbose=False)\n",
        "        ]\n",
        "    )\n",
        "    pred = model.predict(x_valid)\n",
        "\n",
        "    return pred, model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validationで学習/推論(学習専用データを設定可能\n",
        "def train_cv(x_cross, y_cross, x_train, y_train, group, params, n=5, random_state=41):\n",
        "    models = []\n",
        "    oof = np.zeros(len(y_cross), dtype=np.float64)\n",
        "    kf = StratifiedGroupKFold(n_splits=n, shuffle=True, random_state=random_state)\n",
        "    for idx_train, idx_valid in kf.split(x_cross,x_cross['subject'],group):\n",
        "        pred, model = train_holdout(\n",
        "            pd.concat([x_cross.iloc[idx_train], x_train], axis=0),\n",
        "            pd.concat([y_cross.iloc[idx_train], y_train], axis=0),\n",
        "            x_cross.iloc[idx_valid],\n",
        "            y_cross.iloc[idx_valid],\n",
        "            params\n",
        "        )\n",
        "        models.append(model)\n",
        "        oof[idx_valid] = pred\n",
        "    return oof, models"
      ],
      "metadata": {
        "id": "4lJgGSc78fV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7OYp-N2qnzP"
      },
      "outputs": [],
      "source": [
        "# スロープ上り下り分離CVによる学習/推論\n",
        "def train_predict(tr_emg_df, tr_vel_df, ts_emg_df, target_col, drop_cols, params, period=6, n_loop=1):\n",
        "    fd = tr_vel_df['dir']\n",
        "    f5 = tr_emg_df['subject'] == 5\n",
        "    f6 = tr_emg_df['subject'] == 6\n",
        "\n",
        "    train_cols = tr_emg_df.columns[~tr_emg_df.columns.isin(drop_cols)]\n",
        "    tr_pred = 0\n",
        "    ts_pred = 0\n",
        "    models = []\n",
        "    for i in tqdm(range(n_loop)):\n",
        "        seed_everything(41+i)\n",
        "        params['random_state'] = 41+i\n",
        "\n",
        "        tr_pred_sub = np.zeros(len(tr_emg_df), dtype=np.float64)\n",
        "        ts_pred_sub = np.zeros(len(ts_emg_df), dtype=np.float64)\n",
        "        models_sub = []\n",
        "        for time in tqdm(range(0,30,period)):\n",
        "            ftr = (tr_emg_df['time'] >= time) & (tr_emg_df['time'] < time+period)\n",
        "            fts = (ts_emg_df['time'] >= time) & (ts_emg_df['time'] < time+period)\n",
        "            models_time = []\n",
        "\n",
        "            # cross validation (上り)\n",
        "            f = fd & ~f5 & ~f6\n",
        "            group   = tr_emg_df.loc[ f & ftr, 'trial']\n",
        "            x_cross = tr_emg_df.loc[ f & ftr, train_cols]\n",
        "            y_cross = tr_vel_df.loc[ f & ftr, target_col]\n",
        "            x_train = tr_emg_df.loc[~f & ftr, train_cols]\n",
        "            y_train = tr_vel_df.loc[~f & ftr, target_col]\n",
        "            pred, models1 = train_cv(\n",
        "                x_cross, y_cross, x_train, y_train, group, params,random_state=41+i\n",
        "            )\n",
        "            tr_pred_sub[f & ftr] = pred\n",
        "            models_time += models1\n",
        "\n",
        "            # cross validation (下り)\n",
        "            f = ~fd & ~f5 & ~f6\n",
        "            group   = tr_emg_df.loc[ f & ftr, 'trial']\n",
        "            x_cross = tr_emg_df.loc[ f & ftr, train_cols]\n",
        "            y_cross = tr_vel_df.loc[ f & ftr, target_col]\n",
        "            x_train = tr_emg_df.loc[~f & ftr, train_cols]\n",
        "            y_train = tr_vel_df.loc[~f & ftr, target_col]\n",
        "            pred, models2 = train_cv(\n",
        "                x_cross, y_cross, x_train, y_train, group, params,random_state=41+i\n",
        "            )\n",
        "            tr_pred_sub[f & ftr] = pred\n",
        "            models_time += models2\n",
        "\n",
        "            # holdout (リファレンスtrain側を予測)\n",
        "            x_train = tr_emg_df.loc[~f5 & ftr, train_cols]\n",
        "            y_train = tr_vel_df.loc[~f5 & ftr, target_col]\n",
        "            x_valid = tr_emg_df.loc[ f5 & ftr, train_cols].copy()\n",
        "            y_valid = tr_vel_df.loc[ f5 & ftr, target_col]\n",
        "            x_valid['subject'] = 6\n",
        "            pred, model = train_holdout(x_train, y_train, x_valid, y_valid, params)\n",
        "            tr_pred_sub[f5 & ftr] = pred\n",
        "            models_time.append(model)\n",
        "\n",
        "            # holdout (リファレンスtest側を予測)\n",
        "            x_train = tr_emg_df.loc[~f6 & ftr, train_cols]\n",
        "            y_train = tr_vel_df.loc[~f6 & ftr, target_col]\n",
        "            x_valid = tr_emg_df.loc[ f6 & ftr, train_cols].copy()\n",
        "            y_valid = tr_vel_df.loc[ f6 & ftr, target_col]\n",
        "            x_valid['subject'] = 5\n",
        "            pred, model = train_holdout(x_train, y_train, x_valid, y_valid, params)\n",
        "            tr_pred_sub[f6 & ftr] = pred\n",
        "            models_time.append(model)\n",
        "\n",
        "            # 全モデルでtestデータ推論\n",
        "            x_test = ts_emg_df.loc[fts, train_cols]\n",
        "            ts_pred_sub[fts] = 0\n",
        "            for model in models_time:\n",
        "                ts_pred_sub[fts] += model.predict(x_test.values)\n",
        "            ts_pred_sub[fts] /= len(models_time)\n",
        "\n",
        "            # 予測値追加\n",
        "            for t in range(0,30,period):\n",
        "                tr_emg_df.loc[\n",
        "                    (tr_emg_df['time'] >= t) & (tr_emg_df['time'] < t+period), f'pred{time}'\n",
        "                ] = tr_pred_sub[ftr]\n",
        "                ts_emg_df.loc[\n",
        "                    (ts_emg_df['time'] >= t) & (ts_emg_df['time'] < t+period), f'pred{time}'\n",
        "                ] = ts_pred_sub[fts]\n",
        "\n",
        "            print(mean_squared_error(tr_vel_df.loc[ftr, target_col].values, tr_pred_sub[ftr], squared=False))\n",
        "            models_sub += models_time\n",
        "\n",
        "        print(mean_squared_error(tr_vel_df[target_col].values, tr_pred_sub, squared=False))\n",
        "        tr_pred += tr_pred_sub / n_loop\n",
        "        ts_pred += ts_pred_sub / n_loop\n",
        "        models += models_sub\n",
        "\n",
        "        # 追加した予測値をクリア\n",
        "        pred_cols = [f'pred{t}' for t in range(0,30,period)]\n",
        "        tr_emg_df.drop(pred_cols, axis=1, inplace=True)\n",
        "        ts_emg_df.drop(pred_cols, axis=1, inplace=True)\n",
        "\n",
        "    return tr_pred, ts_pred, models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I05Q3Vyiy4u"
      },
      "source": [
        "## 各速度の学習/推論"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izhj_7VFPwjc"
      },
      "outputs": [],
      "source": [
        "n_loop = 1\n",
        "period = 6\n",
        "target_cols = ['vel_z','vel_x','vel_y','vel_r','vel_s','vel_c','vel_e']\n",
        "drop_cols = ['trial','time']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_JqgsuHbe4e"
      },
      "outputs": [],
      "source": [
        "tr_preds = []\n",
        "ts_preds = []\n",
        "model_dict = {}\n",
        "for target_col in target_cols:\n",
        "    # 学習/推論\n",
        "    tr_pred, ts_pred, models = train_predict(\n",
        "        tr_emg_df, tr_vel_df, ts_emg_df,\n",
        "        target_col, drop_cols, params, period=period, n_loop=n_loop\n",
        "    )\n",
        "\n",
        "    # スコア計算\n",
        "    f = tr_vel_df['subject'] <= 4\n",
        "    score1 = mean_squared_error(tr_vel_df.loc[f, target_col].values, tr_pred[f], squared=False)\n",
        "    f = tr_vel_df['subject'] == 5\n",
        "    score2 = mean_squared_error(tr_vel_df.loc[f, target_col].values, tr_pred[f], squared=False)\n",
        "    f = tr_vel_df['subject'] == 6\n",
        "    score3 = mean_squared_error(tr_vel_df.loc[f, target_col].values, tr_pred[f], squared=False)\n",
        "    print(target_col, score1, score2, score3, (score1 + score2 + score3) / 3)\n",
        "\n",
        "    # z軸推論値は特徴量に追加\n",
        "    if target_col[-1] == 'z':\n",
        "        tr_emg_df[f'pred_{target_col}'] = tr_pred\n",
        "        ts_emg_df[f'pred_{target_col}'] = ts_pred\n",
        "\n",
        "    # 結果格納\n",
        "    tr_preds.append(pd.Series(tr_pred, name=f'pred_{target_col}'))\n",
        "    ts_preds.append(pd.Series(ts_pred, name=f'pred_{target_col}'))\n",
        "    model_dict[target_col] = models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxjMzgR0kyYl"
      },
      "source": [
        "# 保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQQoNDe5kQcQ"
      },
      "outputs": [],
      "source": [
        "tr_pred_vel_df = pd.concat(tr_preds, axis=1)\n",
        "tr_pred_vel_df.to_pickle(WORKING / 'tr_pred_vel_fc_p6.pickle')\n",
        "ts_pred_vel_df = pd.concat(ts_preds, axis=1)\n",
        "ts_pred_vel_df.to_pickle(WORKING / 'ts_pred_vel_fc_p6.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "with open(WORKING / 'lgbm_vel_fc_p6.joblib', 'wb') as f:\n",
        "    joblib.dump(model_dict, f, compress=4)"
      ],
      "metadata": {
        "id": "Vrt_aNYBKkuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}